#!/usr/bin/env python3
"""
run_pipeline.py â€” Orchestrateur Docker pour le pipeline ETL complet.

ChaÃ®ne automatiquement :
  1. transform.py  (donnÃ©es brutes â†’ JSONL unifiÃ©)
  2. load_mongodb.py (JSONL â†’ MongoDB avec validation)

Ce script est conÃ§u pour tourner dans le conteneur Docker.
Il rÃ©utilise les scripts standalone sans les modifier :
  - transform.py est appelÃ© via subprocess (comme en CLI)
  - load_mongodb.py est appelÃ© via subprocess (comme en CLI)

Variables d'environnement attendues (via docker-compose) :
  MONGO_URI          - URI MongoDB (ex: mongodb://mongodb:27017/?replicaSet=rs0)
  DATA_ROOT          - Dossier des fichiers sources (montÃ© via volume)

Optionnelles :
  DB_NAME            - Nom de la base (dÃ©faut: weather_db)
  COLLECTION_NAME    - Nom de la collection (dÃ©faut: weather_data)
  FORCE_DIRECT_CONNECTION - true/false (dÃ©faut: false)
"""

import logging
import os
import subprocess
import sys
from pathlib import Path

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(name)s - %(message)s",
)
logger = logging.getLogger("run_pipeline")

SCRIPTS_DIR = Path(__file__).resolve().parent


def run_step(name: str, cmd: list[str]) -> None:
    """ExÃ©cute une commande et lÃ¨ve une erreur si elle Ã©choue."""
    logger.info("=" * 60)
    logger.info(f"Ã‰TAPE : {name}")
    logger.info(f"CMD   : {' '.join(cmd)}")
    logger.info("=" * 60)

    result = subprocess.run(cmd, cwd=str(SCRIPTS_DIR))

    if result.returncode != 0:
        logger.error(f"Ã‰CHEC : {name} (code {result.returncode})")
        sys.exit(result.returncode)

    logger.info(f"OK : {name}")


def main():
    logger.info("ðŸš€ Pipeline ETL Docker â€” Forecast 2.0")

    data_root = os.getenv("DATA_ROOT", "/app/data")
    output_dir = os.getenv("OUTPUT_DIR", "/app/output")
    mongo_uri = os.getenv("MONGO_URI", "mongodb://mongodb:27017/?replicaSet=rs0")

    jsonl_path = f"{output_dir}/weather_data.jsonl"
    report_path = f"{output_dir}/mongodb_report.json"

    # VÃ©rifier que les fichiers sources sont prÃ©sents
    data_path = Path(data_root)
    if not data_path.exists():
        logger.error(f"DATA_ROOT introuvable : {data_root}")
        logger.error("VÃ©rifiez le montage du volume Docker.")
        sys.exit(1)

    # ---- Ã‰TAPE 1 : TRANSFORMATION ----
    run_step(
        "Transformation des donnÃ©es",
        [
            sys.executable,
            str(SCRIPTS_DIR / "transform.py"),
            "--data-root", data_root,
            "--output", jsonl_path,
        ],
    )

    # ---- Ã‰TAPE 2 : IMPORT MONGODB ----
    # On passe la config via les variables d'environnement
    # que load_mongodb.py lit dÃ©jÃ  (MONGO_URI, DB_NAME, etc.)
    env = os.environ.copy()
    env["MONGO_URI"] = mongo_uri
    env["INPUT_PATH"] = jsonl_path
    env["REPORT_PATH"] = report_path
    env["RESET_COLLECTION"] = "true"

    logger.info("=" * 60)
    logger.info("Ã‰TAPE : Import MongoDB")
    logger.info(f"CMD   : {sys.executable} load_mongodb.py")
    logger.info("=" * 60)

    result = subprocess.run(
        [sys.executable, str(SCRIPTS_DIR / "load_mongodb.py")],
        cwd=str(SCRIPTS_DIR),
        env=env,
    )

    if result.returncode != 0:
        logger.error(f"Ã‰CHEC : Import MongoDB (code {result.returncode})")
        sys.exit(result.returncode)

    logger.info("=" * 60)
    logger.info(f"âœ… Pipeline terminÃ© â€” Rapport : {report_path}")
    logger.info("=" * 60)


if __name__ == "__main__":
    main()
